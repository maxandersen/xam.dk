= Illusion of Hype â€” Why Appleâ€™s Paper Doesnâ€™t Mean LLMs Are Useless
:page-layout: post
ifdef::env-github,env-browser,env-vscode[:imagesdir: ../] 

Apple's recent paper, https://ml-site.cdn-apple.com/papers/the-illusion-of-thinking.pdf[*The Illusion of Thinking*], has sparked a wave of discussions both online and offline. Some have latched onto it as a definitive "gotcha" moment against large language models (LLMs), claiming they're nothing but fancy statistical engines that can't truly reason. 

While the paper does a solid job showing that LLMsâ€™ reasoning is, indeed, based on patterns rather than explicit algorithmic logic, thatâ€™s hardly a revelation. Anyone who has studied how LLMs are built knows theyâ€™re made from **neural networks, statistics, and a lot of big data plus compute**â€”not hard-coded algorithms or innate human-like reasoning. Of course they arenâ€™t truly â€œreasoningâ€ in a symbolic sense. But thatâ€™s **exactly** what makes their accomplishments so remarkable: they simulate reasoning so well that they can tackle tasks we once thought only humans could do.

---

## The Real Illusion: The Hype

The *real* illusion is not in the models themselvesâ€”itâ€™s in the hype cycle that surrounds them. Business leaders and CEOs often hype LLMs as though theyâ€™re fully capable, plug-and-play, general reasoning engines. Thatâ€™s not how these systems work, and Appleâ€™s paper simply reinforces what every ML researcher has known from the start: these models are fundamentally **pattern-matching systems**, powered by vast amounts of data and compute.

But donâ€™t mistake that as a condemnation of their usefulness. Itâ€™s a testament to just how far statistical learning has taken usâ€”and how close we are to building tools that can augment human cognition in ways weâ€™ve never seen before.

---

## LLMs: The Most Algorithmic Weâ€™ve Ever Been

Despite their statistical underpinnings, todayâ€™s LLMs come closer to *algorithmic reasoning* than any previous machine learning system. They can:

- **Guide a blind person through the world**, describing scenes and reading labels.
- **Serve as a coding assistant**, helping software developers build and debug faster.
- **Summarize vast troves of information**, distilling knowledge that would otherwise take hours to sift through.

These are not trivial achievements. Theyâ€™re changing lives and workflows right now.

---

## The â€œCollapse at Scaleâ€ Isnâ€™t New â€” And Itâ€™s Not the End

Yes, Appleâ€™s study shows that LLMs fail when faced with extremely complex tasks, like a hundred-step Tower of Hanoi puzzle. But guess what? Most humans would fail there too. Even if an LLMâ€™s chain-of-thought collapses at scale, it **still** does remarkably well at medium complexityâ€”a sweet spot that covers most real-world use cases.

---

## Progress Never Starts Fully Formed

Weâ€™ve seen this story before. The airplane couldnâ€™t fly as far as a bird at first. Computers couldnâ€™t think like a human at first. Every breakthrough technology is at first considered either underwhelming or threatening, and then, as we learn to harness it, society adapts and grows stronger.

In his classic talk *The Future of Programming* (presented in 2013 as if from 1973), Bret Victor reminded us that the tools and ideas we build today will shape the next generation of creativity and progressâ€”even if theyâ€™re not yet fully mature. Just like the early computing pioneers couldnâ€™t imagine where GUIs, networking, and interactive systems would take us, we canâ€™t yet foresee where LLMs and the new wave of reasoning models will lead.

Victorâ€™s talk is a masterclass in humility about the **incremental nature of innovation**â€”how each generation builds on the last, always feeling a bit dissatisfied with the present but full of hope for the future.

---

## Appleâ€™s Paper is a Reminder, Not a Rebuttal

Appleâ€™s paper is a great reminder that LLMs are not magic, and we shouldnâ€™t expect them to perfectly reason through every problem. But itâ€™s not a rebuttal of their usefulness. Itâ€™s a call to understand them betterâ€”and to build tools that make their strengths more accessible while acknowledging their weaknesses.

---

## Letâ€™s Keep Moving Forward

LLMs arenâ€™t the final answer to reasoning, but they are the closest weâ€™ve ever come. They represent the culmination of decades of work in machine learning and language modeling. Letâ€™s celebrate what they can doâ€”and letâ€™s keep building, learning, and evolving together.

As Bret Victor might say: **itâ€™s not about the tool itself, but what it lets us *become* as creators, thinkers, and problem-solvers.**

---

*Do you agree? Have a different perspective? Letâ€™s keep the conversation going in the comments below!*

---

ğŸ“º **Bonus:** If you havenâ€™t seen Bret Victorâ€™s â€œThe Future of Programmingâ€ talk, itâ€™s a must-watch for anyone interested in how we shape the future of technology:
[Watch it here](https://www.youtube.com/watch?v=gbHZNRda08o).

Have fun!

-- Max Rydahl Andersen